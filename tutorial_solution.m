%% Princeton MVPA toolbox tutorial script
% Refer to this online wiki for more detailed instructions
% https://github.com/Huang-Shenyang/princeton-mvpa-toolbox/wiki

%% Setup 
% change working directory
clear
clc

% Check file structure and save path to data
data_dir = 'nifti_set/working_set/'; 


%% Install the toolbox
% add toolbox to MATLAB's path
addpath /share/apps/princeton-mvpa-toolbox-master
addpath('E:\OneDrive - Duke University\Duke\Research\Resources\princeton-mvpa-toolbox');
addpath('E:\OneDrive - Duke University\Duke\Research\Resources\spm12');

% add relevent subfolders
mvpa_add_paths();


%% Introduction
% read wiki

% A quote from the original tutorial:
% "We called this document ''tutorial_easy.htm''. Don't let that fool you. 
% We've really done everything we can to trip you up with it. Just wait 
% till you try TutorialAdv. It's so difficult and scary that small children 
% will cry around the globe whenever a copy is downloaded. In the meantime, 
% let us begin."


%% Initializing the subj structure
% start by creating an empty subj structure
subj = init_subj('haxby8','tutorial_subj');
% to find out more about these functions, use `help <function name>`
help init_subj

% use summarize(subj) to examine what data we have in MATLAB
summarize(subj)


%% Task 1
% Using fsleyes, overlay the mask 'VT_category-selective' on the structural 
% image provided and take a screenshot. Explain the purpose of this mask in 
% terms of the research question and if you see any issues with using this 
% mask. 


%% Load ventral temporal GLM mask
% create the mask that will be used when loading in the data
subj = load_spm_mask(subj,'VT_category-selective', ['nifti_set/working_set/', 'mask_cat_select_vt.nii']);
summarize(subj)


%% Load fMRI data
for i=1:10
    raw_filenames{i} = [data_dir 'haxby8_r' num2str(i) '.nii']; 
end
subj = load_spm_pattern(subj,'epi','VT_category-selective',raw_filenames);

%% Task 2
% In the output of `summarize(subj)`, what is Patterns-'epi'? What does the 
% 577 x 1210 matrix contain? Explain the information by looking at each row 
% and at each column.
summarize(subj)


%% Label the functional data
% aka Condition regressors
subj = init_object(subj,'regressors','conds');
load([data_dir 'tutorial_regs']);
subj = set_mat(subj,'regressors','conds',regs);
condnames = {'face','house','cat','bottle','scissors','shoe','chair','scramble'};
subj = set_objfield(subj,'regressors','conds','condnames',condnames);


%% Task 3
% Look at the conditions by either looking into the regs variable or in 
% MATLAB workspace or visualize it using `imagesc(regs)`.
% Take a screenshot and explain what you can tell about the task structure. 
regs;
figure; imagesc(regs) 


%% Task run information
subj = init_object(subj, 'selector', 'runs');
load([data_dir 'tutorial_runs']);
subj = set_mat(subj,'selector', 'runs', runs);


%% Pre-classification processing
%% z-scoring each run
subj = zscore_runs(subj,'epi','runs');

% confirm that you've indeed created another 'pattern' by looking at
% subj.patterns or using summarize(subj).
% The zscored patten is called 'epi_z'.

% to confirm z-scoring is done properly, examine this scatter plot
figure; scatter(subj.patterns{1}.mat(577,:)',subj.patterns{2}.mat(577,:)');


%% Creating the cross-validation indices
subj_TRselector = create_xvalid_indices(subj,'runs');
summarize(subj_TRselector, 'objtype', 'selector');


%% Task 5
% Explain what is plotted in this figure and why we need this variable.
figure; imagesc(subj_TRselector.selectors{3}.mat); colorbar;


%% Feature selection 
% ANOVA is run multiple times, separately for each iteration
% p-value threshold has a default of 0.05
subj_fs_05 = feature_select(subj_TRselector, 'epi_z', 'conds', 'runs_xval');
subj_fs_001 = feature_select(subj_TRselector, 'epi_z', 'conds', 'runs_xval', 'thresh', 0.001);


% look at selected features and compare different threshold
% Note that 10 additional masks were generated by this function call,
% corresponding to 10 feature selections.
% Examine the masks 
summarize(subj_fs_05,'objtype','mask')

% % Suppose you don't know that the toolbox has a function to print out that
% % information nicely, just construct a simple for loop
% for i=2:11
%     % i
%     % subj_fs_05.masks{i}
%     subj_fs_05.masks{i}.nvox
%     subj_fs_001.masks{i}.nvox
% end


%% Task 6
% How many voxels are included in each of the newly generated masks, with a 
% p-value threshold set to 0.001?
% Why did ANOVA only remove a very small number of voxels in each case?


%% Leave-one-run-out cross-validation classification
% set some basic arguments for a backprop classifier
clf = 'bp'; % backpropogation neural network 
% clf = 'ridge'; % ridge regression
% clf = 'logreg'; % logistic regression
% clf = 'svm'; % Though theoretically SVM can classify multiple categories, this implementation cannot. 
class_args.train_funct_name = ['train_' clf]; % these are the actual classifers used for classification
class_args.test_funct_name = ['test_' clf];

% parameters for the classifier chosen
class_args.nHidden = 0; % number of hidden layers in bp neural network
% class_args.penalty = 20; % parameter for logistic regression and ridge regression to penalize small weights
% class_args.kernel_type = 2; % kernel for SVM which doesn't work here

% Note that classification performance will be different because backprop randomly initializes its weights each time it is run.

% now, run the classification multiple times, training and testing
% on different subsets of the data on each iteration
[subj_fs_05, results_fs_05] = cross_validation(subj_fs_05,'epi_z','conds','runs_xval','epi_z_thresh0.05',class_args);
% [subj_fs_001, results_fs_001] = cross_validation(subj_fs_001,'epi_z','conds','runs_xval','epi_z_thresh0.001',class_args);


%% Warning: Not 1-of-n regressors 
% Apparently, the toolbox creators deliberately left the rest TRs in the
% tutorial dataset.
figure; imagesc(sum(regs)); colorbar;


%% Remove rest TRs from selectors
% identify rest TRs
temp_sel = zeros(1,size(regs,2));
temp_sel(sum(regs)==1) = 1;

% create a new 'no_rest' selector object
subj_TRselector_norest = init_object(subj,'selector','no_rest');
subj_TRselector_norest = set_mat(subj_TRselector_norest,'selector','no_rest',temp_sel);
subj_TRselector_norest = create_xvalid_indices(subj_TRselector_norest,'runs','actives_selname','no_rest');

% feature selection
subj_norest_fs_05 = feature_select(subj_TRselector_norest,'epi_z','conds','runs_xval');

% classification
[subj_norest_fs_05, results_norest_fs_05] = cross_validation(subj_norest_fs_05,'epi_z','conds','runs_xval','epi_z_thresh0.05',class_args);


%% Task 7
% Why does classification accuracy improve when we exclude rest TRs?
results_fs_05.total_perf
results_norest_fs_05.total_perf


%% Concluding remarks
% Again, quote from the original toolbox wiki:
% Congratulations on completing this tutorial. Now you know
% enough to be dangerous. Play with your data for a bit, and
% then try TutorialAdv which builds on all these
% concepts and introduces further procedures for the kinds of
% things that you'll probably want to try before too long.

% For instance, we aren't convolving our regressors with a
% haemodynamic response function to take account of the lag in
% the BOLD response - this is a pretty critical omission.


