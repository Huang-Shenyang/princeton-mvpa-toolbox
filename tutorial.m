%% Princeton MVPA toolbox tutorial script
% Refer to this online wiki for more detailed instructions
% https://github.com/Huang-Shenyang/princeton-mvpa-toolbox/wiki


%% Setup 
% change working directory

% Check file structure and save path to data


%% Install the toolbox
% add toolbox to MATLAB's path

% add relevent subfolders


%% Introduction
% read wiki

% A quote from the original tutorial:
% "We called this document ''tutorial_easy.htm''. Don't let that fool you. 
% We've really done everything we can to trip you up with it. Just wait 
% till you try TutorialAdv. It's so difficult and scary that small children 
% will cry around the globe whenever a copy is downloaded. In the meantime, 
% let us begin."


%% Initializing the subj structure
% start by creating an empty subj structure

% to find out more about these functions, use `help <function name>`

% use summarize(subj) to examine what data we have in MATLAB


%% Task 1
% Using fsleyes, overlay the mask 'VT_category-selective' on the structural 
% image provided and take a screenshot. Explain the purpose of this mask in 
% terms of the research question and if you see any issues with using this 
% mask. 


%% Load ventral temporal GLM mask
% create the mask that will be used when loading in the data


%% Load fMRI data


%% Task 2
% In the output of `summarize(subj)`, what is Patterns-'epi'? What does the 
% 577 x 1210 matrix contain? Explain the information by looking at each row 
% and at each column.


%% Label the functional data
% aka Condition regressors


%% Task 3
% Look at the conditions by either looking into the regs variable or in 
% MATLAB workspace or visualize it using `imagesc(regs)`.
% Take a screenshot and explain what you can tell about the task structure. 


%% Task run information


%% Pre-classification processing
%% z-scoring each run

% confirm that you've indeed created another 'pattern' by looking at
% subj.patterns or using summarize(subj).
% The zscored patten is called 'epi_z'.

% to confirm z-scoring is done properly, examine this scatter plot


%% Creating the cross-validation indices


%% Task 5
% Explain what is plotted in this figure and why we need this variable.
figure; imagesc(subj_TRselector.selectors{3}.mat); colorbar;


%% Feature selection 
% ANOVA is run multiple times, separately for each iteration
% p-value threshold has a default of 0.05


% look at selected features and compare different threshold
% Note that 10 additional masks were generated by this function call,
% corresponding to 10 feature selections.
% Examine the masks 


% % Suppose you don't know that the toolbox has a function to print out that
% % information nicely, just construct a simple for loop
% for i=2:11
%     % i
%     % subj_fs_05.masks{i}
%     subj_fs_05.masks{i}.nvox
%     subj_fs_001.masks{i}.nvox
% end


%% Task 6
% How many voxels are included in each of the newly generated masks, with a 
% p-value threshold set to 0.001?
% Why did ANOVA only remove a very small number of voxels in each case?


%% Leave-one-run-out cross-validation classification
% set some basic arguments for a backprop classifier
clf = 'bp'; % backpropogation neural network 
% clf = 'ridge'; % ridge regression
% clf = 'logreg'; % logistic regression
% clf = 'svm'; % Though theoretically SVM can classify multiple categories, this implementation cannot. 
class_args.train_funct_name = ['train_' clf]; % these are the actual classifers used for classification
class_args.test_funct_name = ['test_' clf];

% parameters for the classifier chosen
class_args.nHidden = 0; % number of hidden layers in bp neural network
% class_args.penalty = 20; % parameter for logistic regression and ridge regression to penalize small weights
% class_args.kernel_type = 2; % kernel for SVM which doesn't work here

% Note that classification performance will be different because backprop randomly initializes its weights each time it is run.

% now, run the classification multiple times, training and testing
% on different subsets of the data on each iteration


%% Warning: Not 1-of-n regressors 
% Apparently, the toolbox creators deliberately left the rest TRs in the
% tutorial dataset.


%% Remove rest TRs from selectors


%% Task 7
% Why does classification accuracy improve when we exclude rest TRs?


%% Concluding remarks
% Again, quote from the original toolbox wiki:
% Congratulations on completing this tutorial. Now you know
% enough to be dangerous. Play with your data for a bit, and
% then try TutorialAdv which builds on all these
% concepts and introduces further procedures for the kinds of
% things that you'll probably want to try before too long.

% For instance, we aren't convolving our regressors with a
% haemodynamic response function to take account of the lag in
% the BOLD response - this is a pretty critical omission.

